# Benchmarking Energy Consumption for LLM Inference Across Diverse Workloads
*Group Members: Sri Iyengar, Anushka Pachary, Radhika Patel*
*Mentored by: Jishan Desai, Rakene Chowdhary*

---

## ğŸ“‹ Overview

This project provides a standardized benchmarking suite to evaluate the energy efficiency of Large Language Models (LLMs) during inference. By systematically varying inference workloadsâ€”particularly **input length, output length, quantization type, batch size and dataset**, we measure their impact on: 
- latency (sec)
- Max GPU Memory Usage (MB)
- Total Energy Consumption (Joules)
- Carbon Footprint (gCOâ‚‚eq)
- Energy per input/output token

across **different models**. 

The benchmarking pipeline is implemented using Hugging Face Transformers, bitsandbytes for quantization, Zeus for power profiling. Metrics are visualized and tracked via Weights & Biases (W&B) and optionally logged into csv files locally. 

### Motivation

LLMs like LLaMA-2 and DeepSeek require significant energy for inference. However, few tools exist to rigorously benchmark and compare their energy profiles across realistic workloads (and configurations).

This project builds a reproducible pipeline to:
- Run controlled inference experiments across models, datasets, and quantization levels.
- Monitor real-time GPU power usage and calculate associated energy and carbon footprint.
- Log detailed metrics (latency, memory, energy per token, carbon impact).
- Visualize performance across sweeping variables like input/ouput size, model type, dataset type, batch size.

--- 

### Installation 

We recommend setting up a virtual environment before installing dependencies:

```
python -m venv venv
source venv/bin/activate       # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```
Make sure nvidia-smi and GPU drivers are properly installed if running on an NVIDIA system.


### How To Benchmark

The main entry point is scripts/evaluate.py, which:

- Loads datasets, defines model configs and sweep params 

- Sweeps one variable at a time across options like:

    - input_length: short (128), medium (512), long (1024)
    - output_length: same as above
    - batch_size: 1, 2, 4
    - quantization: int4, int8, fp16
    - dataset: Alpaca, GSM8K
    - model: Llama-7b, Deepseek-R1-Distill-Qwen-7B

- Logs metrics to visual plots in W&B

### Run Benchmark
```
python scripts/evaluate.py
```
---

### Power Monitoring Tools
This project supports two power monitoring backends:

1. Zeus (default): Used live inside evaluate.py and testing.py.

2. nvidia-smi logging: Via metrics/monitor_gpu.py (manual alternative).

The file carbon_utils.py uses real-time carbon intensity data via the [Electricity Maps API](https://portal.electricitymaps.com/docs/getting-started#authorization).


### Datasets

We use two open-source datasets for prompting:

* ğŸ¦™ Alpaca â€“ instruction-following prompts

* ğŸ“ GSM8K â€“ math and reasoning tasks

See utils/data.py for how these are preprocessed and sampled.

---

### Directory Structure

```
â”œâ”€â”€ metrics/                 # Energy and power monitoring utilities
â”‚   â”œâ”€â”€ monitor_gpu.py       # (For manual tracking) nvidia-smi based GPU power logger
â”‚   â”œâ”€â”€ parse_power_log.py   # (For manual logging) CSV power log parser
â”‚   â””â”€â”€ zeusml.py            # Wrapper for Zeus energy monitor
â”‚
â”œâ”€â”€ results/                 # Output metrics and plots
â”‚   â”œâ”€â”€ gpu_power_log.csv    # Raw GPU power logging (from nvidia-smi)
â”‚   â”œâ”€â”€ latency_plot.png
â”‚   â”œâ”€â”€ memory_plot.png
â”‚   â”œâ”€â”€ metrics_output.csv   # Master log of all experiment results
â”‚   â””â”€â”€ power_plot.png
â”‚
â”œâ”€â”€ scripts/                 # Core experiment driver and plot scripts
â”‚   â”œâ”€â”€ evaluate.py          # Main script for controlled benchmarking experiments
â”‚   â””â”€â”€ plot_results.py      # (Deprecated) Static bar plot generation for Llama
â”‚
â”œâ”€â”€ utils/                   # Helper modules
â”‚   â”œâ”€â”€ carbon_utils.py      # Converts energy to CO2eq using real-time carbon intensity API
â”‚   â”œâ”€â”€ data.py              # Loads and cleans datasets (Alpaca, GSM8K)
â”‚   â”œâ”€â”€ load_model.py        # Loads Hugging Face models with quantization
â”‚   â””â”€â”€ testing.py           # Runs sweep-based experiments and logs results
â”‚
â”œâ”€â”€ wandb/                   # Dir created for wandb runs
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt         # Python package dependencies
```

### Future Work:

- Increase memory to try baseline fp32 quantization level 
- Benchmarking across multiple hardware configurations